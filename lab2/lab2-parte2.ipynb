{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57knM8jrYZ2t"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eirasf/GCED-AA2/blob/main/lab2/lab2-parte2.ipynb)\n",
        "\n",
        "# Práctica 1: Redes neuronales desde cero con TensorFlow - Parte 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU7bXtcZD3Qp"
      },
      "source": [
        "En esta segunda parte de la práctica vamos a utilizar TensorFlow para implementar y entrenar la misma red neuronal que desarrollamos con Numpy en la parte 1.\n",
        "\n",
        "Necesitaremos, por tanto, la librería TensorFlow además de las ya utilizadas Numpy y tensorflow_datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LkaimNJfYZ2w",
        "outputId": "38e5f05a-23b2-46b1-8534-cd47ef80342c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "# COLAB - Para ejecutar desde Colab, descomentar la siguiente línea\n",
        "%tensorflow_version 2.x\n",
        "# LOCAL - Para ejecutar en Local, descomentar la siguiente línea\n",
        "#!pip3 install tensorflow numpy tensorflow-datasets\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Establecemos una semilla aleatoria para que los resultados sean reproducibles en distintas ejecuciones\n",
        "np.random.seed(1234567)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwFjbUR9D3Qr"
      },
      "source": [
        "Cargaremos el conjunto de datos `german_credit_numeric` del que tomaremos el primer lote de 100 elementos, tal como hicimos en la parte 1 de la práctica. Obtendremos dos tensores (`vectores_x` y `etiquetas`) que serán los que utilizaremos posteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OIc9szkaD3Qr"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Cargamos el conjunto de datos\n",
        "ds = tfds.load('german_credit_numeric', split='train')\n",
        "\n",
        "tamano_lote = 100\n",
        "\n",
        "elems = ds.batch(tamano_lote)\n",
        "lote_entrenamiento = None\n",
        "for elem in elems:\n",
        "    lote_entrenamiento = elem\n",
        "    break\n",
        "\n",
        "vectores_x = tf.cast(lote_entrenamiento[\"features\"], dtype=tf.float64)\n",
        "etiquetas = tf.cast(lote_entrenamiento[\"label\"], dtype=tf.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgwTfKxFD3Qs"
      },
      "source": [
        "## Declaración del modelo\n",
        "\n",
        "En primer lugar, debemos crear en TensorFlow el grafo de operaciones que representa nuestro modelo. Para ello:\n",
        " 1. Creamos las variables que TF optimizará, es decir, los parámetros del modelo.\n",
        " 1. Creamos el grafo de operaciones que producen la predicción a partir de la entrada y las variables. En este caso utilizaremos funciones que relacionen variables de TF con tensores que contendrán datos utilizando operaciones de TF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e_vaZvwnD3Qs",
        "outputId": "3bbb6c03-cfa3-4861-8688-b623e69a94a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.48001507]], shape=(1, 1), dtype=float64)\n"
          ]
        }
      ],
      "source": [
        "# Variables auxiliares\n",
        "tamano_entrada = 24\n",
        "h0_size = 5\n",
        "h1_size = 3\n",
        "\n",
        "# CREACIÓN DE LAS VARIABLES\n",
        "W0 = tf.Variable(np.random.randn(h0_size, tamano_entrada), dtype=tf.float64, name='W0')\n",
        "b0 = tf.Variable(np.random.randn(h0_size, 1), dtype=tf.float64, name='b0')\n",
        "\n",
        "W1 = tf.Variable(np.random.randn(h1_size, h0_size), dtype=tf.float64, name='W1')\n",
        "b1 = tf.Variable(np.random.randn(h1_size, 1), dtype=tf.float64, name='b1')\n",
        "\n",
        "W2 = tf.Variable(np.random.randn(1, h1_size), dtype=tf.float64, name='W2')\n",
        "b2 = tf.Variable(np.random.randn(1, 1), dtype=tf.float64, name='b2')\n",
        "\n",
        "VARIABLES = [W0, b0, W1, b1, W2, b2]\n",
        "\n",
        "# CREACIÓN DEL GRAFO DE OPERACIONES\n",
        "@tf.function\n",
        "def sigmoide(x):\n",
        "    return 1 / (1 + tf.exp(-x))\n",
        "\n",
        "@tf.function\n",
        "def capa_sigmoide(x, W, b):\n",
        "    # b se transpone para que sea (1, n_out) y se pueda sumar por broadcasting\n",
        "    return sigmoide(tf.matmul(x, tf.transpose(W)) + tf.transpose(b))\n",
        "\n",
        "@tf.function\n",
        "def predice(x):\n",
        "    h0 = capa_sigmoide(x, W0, b0)\n",
        "    h1 = capa_sigmoide(h0, W1, b1)\n",
        "    y = capa_sigmoide(h1, W2, b2)\n",
        "    return y\n",
        "\n",
        "\n",
        "# Verificación\n",
        "x_test = np.random.randn(1,tamano_entrada)\n",
        "y_pred = predice(x_test)\n",
        "print(y_pred)\n",
        "np.testing.assert_almost_equal(0.48001507, y_pred.numpy(), err_msg='Revisa tu implementación')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbNnorpeD3Qt"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "El modelo declarado ya se puede utilizar para hacer predicciones pasándole a la función `predice` un tensor con datos (tal como se ha hecho en el apartado de verificación de la celda anterior). Sin embargo, como vimos en la parte 1, este modelo no está ajustado a los datos de entrada, por lo que producirá malas predicciones.\n",
        "\n",
        "Debemos encontrar un conjunto de valores para los parámetros ($\\mathbf{W}_2$, $b_2$, $\\mathbf{W}_1$, $\\mathbf{b}_1$, $\\mathbf{W}_0$ y $\\mathbf{b}_0$) que minimicen la función de coste. TensorFlow nos ayuda a optimizar este proceso.\n",
        "\n",
        "TensorFlow permite configurar el proceso de optimización, por lo que deberemos indicarle:\n",
        " 1. Qué función de pérdida queremos. En nuestro caso habíamos elegido la entropía cruzada binaria.\n",
        " 1. Qué método de optimización utilizar. Como en la parte 1, utilizaremos descenso de gradiente.\n",
        "\n",
        "Por el momento crearemos sendas variables para almacenar ambas configuraciones. Al estar organizado de esta manera, utilizar una función de pérdida distinta o un algoritmo de optimización diferente será tan sencillo como cambiar estas variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UY6Zy7IrD3Qt"
      },
      "outputs": [],
      "source": [
        "fn_perdida = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "optimizador = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "#optimizador = tf.keras.optimizers.Adam(0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NewHUwsQD3Qt"
      },
      "source": [
        "### El bucle de entrenamiento\n",
        "\n",
        "El bucle de entrenamiento será análogo al utilizado en la parte 1. Consistirá en ejecutar un número preestablecido (`NUM_EPOCHS`) de pasos de entrenamiento. En cada paso haremos lo siguiente:\n",
        " 1. Tomar los datos de entrada y calcular las predicciones que hace el modelo en su estado actual\n",
        " 1. Calcular el coste (la media de las pérdidas de cada predicción)\n",
        " 1. Utilizar el valor de coste para actualizar cada variable en dirección de su gradiente\n",
        "\n",
        "Crearemos una función `paso_entrenamiento` que realice este trabajo. TensorFlow se ocupará de calcular los gradientes y realizar las actualizaciones de las variables. Para calcular los gradientes, TensorFlow utiliza un `GradientTape`. Todas las operaciones con tensores que se realicen dentro del entorno en que está declarado este `GradientTape` quedarán registradas y eso nos permitirá obtener los gradientes directamente del `GradientTape` con una simple llamada. Puedes comprobar su funcionamiento en el ejemplo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WpyWOTB8D3Qu",
        "outputId": "599ca6d1-6b21-4581-d881-5d5a8fe9c21a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 99 Pérdida: 0.65055674 Tasa de acierto: 0.5237970133452802\n",
            "Epoch: 199 Pérdida: 0.6124922 Tasa de acierto: 0.5523350431790992\n",
            "Epoch: 299 Pérdida: 0.5955333 Tasa de acierto: 0.5707902872738352\n",
            "Epoch: 399 Pérdida: 0.58786637 Tasa de acierto: 0.5827775341930248\n",
            "Epoch: 499 Pérdida: 0.5843308 Tasa de acierto: 0.5906537392653814\n",
            "Epoch: 599 Pérdida: 0.58265615 Tasa de acierto: 0.5958912321056649\n",
            "Epoch: 699 Pérdida: 0.58182967 Tasa de acierto: 0.5994109381188322\n",
            "Epoch: 799 Pérdida: 0.5813935 Tasa de acierto: 0.6017976062886653\n",
            "Epoch: 899 Pérdida: 0.5811384 Tasa de acierto: 0.6034289972774414\n",
            "Epoch: 999 Pérdida: 0.58096784 Tasa de acierto: 0.6045529112807128\n",
            "Epoch: 1099 Pérdida: 0.5808373 Tasa de acierto: 0.6053338820409074\n",
            "Epoch: 1199 Pérdida: 0.5807259 Tasa de acierto: 0.6058821511222175\n",
            "Epoch: 1299 Pérdida: 0.58062387 Tasa de acierto: 0.6062720693953498\n",
            "Epoch: 1399 Pérdida: 0.58052665 Tasa de acierto: 0.6065540121017616\n",
            "Epoch: 1499 Pérdida: 0.580432 Tasa de acierto: 0.6067622191067037\n",
            "Epoch: 1599 Pérdida: 0.580339 Tasa de acierto: 0.6069200130067306\n",
            "Epoch: 1699 Pérdida: 0.5802471 Tasa de acierto: 0.6070433034451546\n",
            "Epoch: 1799 Pérdida: 0.58015597 Tasa de acierto: 0.6071429537613428\n",
            "Epoch: 1899 Pérdida: 0.5800655 Tasa de acierto: 0.6072263869549869\n",
            "Epoch: 1999 Pérdida: 0.5799756 Tasa de acierto: 0.6072986776309236\n",
            "Epoch: 2099 Pérdida: 0.57988626 Tasa de acierto: 0.6073632971114918\n",
            "Epoch: 2199 Pérdida: 0.5797975 Tasa de acierto: 0.6074226219050595\n",
            "Epoch: 2299 Pérdida: 0.5797092 Tasa de acierto: 0.6074782799536621\n",
            "Epoch: 2399 Pérdida: 0.5796213 Tasa de acierto: 0.6075313878175266\n",
            "Epoch: 2499 Pérdida: 0.57953393 Tasa de acierto: 0.6075827130782312\n",
            "Epoch: 2599 Pérdida: 0.579447 Tasa de acierto: 0.6076327852681574\n",
            "Epoch: 2699 Pérdida: 0.5793604 Tasa de acierto: 0.6076819701520928\n",
            "Epoch: 2799 Pérdida: 0.5792742 Tasa de acierto: 0.6077305229197161\n",
            "Epoch: 2899 Pérdida: 0.5791883 Tasa de acierto: 0.6077786225767545\n",
            "Epoch: 2999 Pérdida: 0.5791029 Tasa de acierto: 0.6078263956510598\n",
            "Epoch: 3099 Pérdida: 0.57901776 Tasa de acierto: 0.6078739325239924\n",
            "Epoch: 3199 Pérdida: 0.57893294 Tasa de acierto: 0.6079212986146666\n",
            "Epoch: 3299 Pérdida: 0.5788485 Tasa de acierto: 0.6079685410858428\n",
            "Epoch: 3399 Pérdida: 0.5787643 Tasa de acierto: 0.6080156940307674\n",
            "Epoch: 3499 Pérdida: 0.5786805 Tasa de acierto: 0.6080627813448274\n",
            "Epoch: 3599 Pérdida: 0.578597 Tasa de acierto: 0.6081098189738869\n",
            "Epoch: 3699 Pérdida: 0.5785138 Tasa de acierto: 0.6081568166397935\n",
            "Epoch: 3799 Pérdida: 0.57843095 Tasa de acierto: 0.6082037793968967\n",
            "Epoch: 3899 Pérdida: 0.5783484 Tasa de acierto: 0.6082507082065591\n",
            "Epoch: 3999 Pérdida: 0.57826614 Tasa de acierto: 0.6082976014352717\n",
            "Epoch: 4099 Pérdida: 0.57818425 Tasa de acierto: 0.6083444559738711\n",
            "Epoch: 4199 Pérdida: 0.5781026 Tasa de acierto: 0.6083912679736979\n",
            "Epoch: 4299 Pérdida: 0.5780212 Tasa de acierto: 0.608438034111687\n",
            "Epoch: 4399 Pérdida: 0.57794017 Tasa de acierto: 0.6084847518479447\n",
            "Epoch: 4499 Pérdida: 0.57785934 Tasa de acierto: 0.6085314198907641\n",
            "Epoch: 4599 Pérdida: 0.5777788 Tasa de acierto: 0.6085780387333428\n",
            "Epoch: 4699 Pérdida: 0.5776984 Tasa de acierto: 0.6086246107801643\n",
            "Epoch: 4799 Pérdida: 0.57761824 Tasa de acierto: 0.6086711395670924\n",
            "Epoch: 4899 Pérdida: 0.5775382 Tasa de acierto: 0.6087176304689365\n",
            "Epoch: 4999 Pérdida: 0.5774583 Tasa de acierto: 0.6087640898119051\n",
            "Epoch: 5099 Pérdida: 0.5773785 Tasa de acierto: 0.6088105250194826\n",
            "Epoch: 5199 Pérdida: 0.5772989 Tasa de acierto: 0.6088569444978623\n",
            "Epoch: 5299 Pérdida: 0.5772193 Tasa de acierto: 0.6089033567042771\n",
            "Epoch: 5399 Pérdida: 0.5771398 Tasa de acierto: 0.608949770566652\n",
            "Epoch: 5499 Pérdida: 0.57706034 Tasa de acierto: 0.6089961953123573\n",
            "Epoch: 5599 Pérdida: 0.5769808 Tasa de acierto: 0.6090426399465968\n",
            "Epoch: 5699 Pérdida: 0.5769014 Tasa de acierto: 0.6090891135132214\n",
            "Epoch: 5799 Pérdida: 0.5768219 Tasa de acierto: 0.6091356248014863\n",
            "Epoch: 5899 Pérdida: 0.5767424 Tasa de acierto: 0.6091821826497629\n",
            "Epoch: 5999 Pérdida: 0.5766628 Tasa de acierto: 0.6092287949619606\n",
            "Epoch: 6099 Pérdida: 0.5765831 Tasa de acierto: 0.6092754698256192\n",
            "Epoch: 6199 Pérdida: 0.5765032 Tasa de acierto: 0.609322214764275\n",
            "Epoch: 6299 Pérdida: 0.5764232 Tasa de acierto: 0.6093690368842162\n",
            "Epoch: 6399 Pérdida: 0.57634306 Tasa de acierto: 0.609415942337286\n",
            "Epoch: 6499 Pérdida: 0.5762626 Tasa de acierto: 0.609462937096104\n",
            "Epoch: 6599 Pérdida: 0.57618195 Tasa de acierto: 0.6095100262540616\n",
            "Epoch: 6699 Pérdida: 0.57610095 Tasa de acierto: 0.6095572132904603\n",
            "Epoch: 6799 Pérdida: 0.5760196 Tasa de acierto: 0.6096045007972551\n",
            "Epoch: 6899 Pérdida: 0.5759378 Tasa de acierto: 0.6096518895032046\n",
            "Epoch: 6999 Pérdida: 0.5758556 Tasa de acierto: 0.6096993780396417\n",
            "Epoch: 7099 Pérdida: 0.5757729 Tasa de acierto: 0.60974696151741\n",
            "Epoch: 7199 Pérdida: 0.5756895 Tasa de acierto: 0.609794631143916\n",
            "Epoch: 7299 Pérdida: 0.57560533 Tasa de acierto: 0.6098423722783329\n",
            "Epoch: 7399 Pérdida: 0.57552046 Tasa de acierto: 0.6098901619691156\n",
            "Epoch: 7499 Pérdida: 0.5754347 Tasa de acierto: 0.6099379664039903\n",
            "Epoch: 7599 Pérdida: 0.5753478 Tasa de acierto: 0.6099857358770288\n",
            "Epoch: 7699 Pérdida: 0.57525945 Tasa de acierto: 0.6100333975640764\n",
            "Epoch: 7799 Pérdida: 0.57516974 Tasa de acierto: 0.610080845083385\n",
            "Epoch: 7899 Pérdida: 0.57507807 Tasa de acierto: 0.6101279221073836\n",
            "Epoch: 7999 Pérdida: 0.5749842 Tasa de acierto: 0.6101743980314245\n",
            "Epoch: 8099 Pérdida: 0.5748874 Tasa de acierto: 0.6102199289202823\n",
            "Epoch: 8199 Pérdida: 0.5747869 Tasa de acierto: 0.6102640004802422\n",
            "Epoch: 8299 Pérdida: 0.5746813 Tasa de acierto: 0.6103058478467385\n",
            "Epoch: 8399 Pérdida: 0.57456875 Tasa de acierto: 0.610344375423332\n",
            "Epoch: 8499 Pérdida: 0.5744459 Tasa de acierto: 0.6103781904865125\n",
            "Epoch: 8599 Pérdida: 0.57430804 Tasa de acierto: 0.6104061776349647\n",
            "Epoch: 8699 Pérdida: 0.574149 Tasa de acierto: 0.6104295729773409\n",
            "Epoch: 8799 Pérdida: 0.57396775 Tasa de acierto: 0.6104556594107992\n",
            "Epoch: 8899 Pérdida: 0.57377714 Tasa de acierto: 0.6104973963225582\n",
            "Epoch: 8999 Pérdida: 0.5735972 Tasa de acierto: 0.6105630472870849\n",
            "Epoch: 9099 Pérdida: 0.57343674 Tasa de acierto: 0.6106487102553317\n",
            "Epoch: 9199 Pérdida: 0.5732935 Tasa de acierto: 0.6107439673142663\n",
            "Epoch: 9299 Pérdida: 0.57316214 Tasa de acierto: 0.610839424108921\n",
            "Epoch: 9399 Pérdida: 0.57303864 Tasa de acierto: 0.610928914795363\n",
            "Epoch: 9499 Pérdida: 0.57291895 Tasa de acierto: 0.6110085003084911\n",
            "Epoch: 9599 Pérdida: 0.5727987 Tasa de acierto: 0.6110745865798427\n",
            "Epoch: 9699 Pérdida: 0.5726691 Tasa de acierto: 0.6111211565165715\n",
            "Epoch: 9799 Pérdida: 0.5725037 Tasa de acierto: 0.6111327247485565\n",
            "Epoch: 9899 Pérdida: 0.57220125 Tasa de acierto: 0.6110655037320311\n",
            "Epoch: 9999 Pérdida: 0.5717838 Tasa de acierto: 0.6109910716565992\n"
          ]
        }
      ],
      "source": [
        "@tf.function\n",
        "def paso_entrenamiento(x, y):\n",
        "    # Declaración del GradientTape que registrará las operaciones\n",
        "    with tf.GradientTape() as tape:\n",
        "        # TODO - Completa la siguiente línea para que calcule las predicciones\n",
        "        y_pred = predice(x) # Forward pass\n",
        "\n",
        "        # Cálculo de la pérdida utilizando la función que hemos escogido anteriormente\n",
        "        perdida = fn_perdida(y, y_pred)\n",
        "\n",
        "        # Consultar los gradientes es tan sencillo como indicarle dos cosas:\n",
        "        #    1. la función cuyo gradiente queremos obtener\n",
        "        #    2. la lista de variables respecto a las cuales queremos calcular el gradiente\n",
        "        # La función nos devolverá una lista con el gradiente correspondiente a cada variable de la lista\n",
        "        gradientes = tape.gradient(perdida, VARIABLES)\n",
        "\n",
        "        # Realizar la actualización de las variables solo requiere esta llamada. Se le pasa una lista de tuplas (gradiente, variable)\n",
        "        optimizador.apply_gradients(zip(gradientes, VARIABLES))\n",
        "\n",
        "        # Para poder mostrar la tasa de acierto, la calculamos a cada paso\n",
        "        fallos = tf.abs(tf.reshape(y,(tamano_lote, 1)) - y_pred)\n",
        "        tasa_acierto = tf.reduce_sum(1 - fallos)\n",
        "\n",
        "        # Devolvemos estos dos valores para poder mostrarlos por pantalla cuando estimemos conveniente\n",
        "        return (perdida, tasa_acierto)\n",
        "\n",
        "\n",
        "\n",
        "# PROCESO DE ENTRENAMIENTO\n",
        "num_epochs = 10000\n",
        "for epoch in range(num_epochs):\n",
        "    perdida, tasa_error = paso_entrenamiento(vectores_x, etiquetas)\n",
        "\n",
        "    if epoch % 100 == 99:\n",
        "        print(\"Epoch:\", epoch, 'Pérdida:', perdida.numpy(), 'Tasa de acierto:', tasa_error.numpy()/tamano_lote)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AndSiQUYD3Qu"
      },
      "source": [
        "El uso de TensorFlow nos ha permitido abstraernos de los detalles de implementación y del cálculo de derivadas para centrarnos en la arquitectura de nuestro modelo."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WBk0ZDWY-ff8"
      ],
      "name": "AA2 - Lab2 - Parte 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
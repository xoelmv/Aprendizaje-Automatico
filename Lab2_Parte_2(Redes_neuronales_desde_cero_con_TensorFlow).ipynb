{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xoelmv/Aprendizaje-Automatico/blob/main/Lab2_Parte_2(Redes_neuronales_desde_cero_con_TensorFlow).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57knM8jrYZ2t"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eirasf/GCED-AA2/blob/main/lab2/lab2-parte2.ipynb)\n",
        "\n",
        "# Práctica 1: Redes neuronales desde cero con TensorFlow - Parte 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG55QUlY2qea"
      },
      "source": [
        "En esta segunda parte de la práctica vamos a utilizar TensorFlow para implementar y entrenar la misma red neuronal que desarrollamos con Numpy en la parte 1.\n",
        "\n",
        "Necesitaremos, por tanto, la librería TensorFlow además de las ya utilizadas Numpy y tensorflow_datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkaimNJfYZ2w"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Establecemos una semilla aleatoria para que los resultados sean reproducibles en distintas ejecuciones\n",
        "np.random.seed(1234567)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKseGr7J2qee"
      },
      "source": [
        "Cargaremos el conjunto de datos `german_credit_numeric` del que tomaremos el primer lote de 100 elementos, tal como hicimos en la parte 1 de la práctica. Obtendremos dos tensores (`vectores_x` y `etiquetas`) que serán los que utilizaremos posteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeqBb2SM2qef"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Cargamos el conjunto de datos\n",
        "ds = tfds.load('german_credit_numeric', split='train')\n",
        "\n",
        "tamano_lote = 100\n",
        "\n",
        "elems = ds.batch(tamano_lote)\n",
        "lote_entrenamiento = None\n",
        "for elem in elems:\n",
        "    lote_entrenamiento = elem\n",
        "    break\n",
        "\n",
        "vectores_x = tf.cast(lote_entrenamiento[\"features\"], dtype=tf.float64)\n",
        "etiquetas = tf.cast(lote_entrenamiento[\"label\"], dtype=tf.float64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTpdyAE22qeg"
      },
      "source": [
        "## Declaración del modelo\n",
        "\n",
        "En primer lugar, debemos crear en TensorFlow el grafo de operaciones que representa nuestro modelo. Para ello:\n",
        " 1. Creamos las variables que TF optimizará, es decir, los parámetros del modelo.\n",
        " 1. Creamos el grafo de operaciones que producen la predicción a partir de la entrada y las variables. En este caso utilizaremos funciones que relacionen variables de TF con tensores que contendrán datos utilizando operaciones de TF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMrlpumB2qeh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf2526f-12e0-4dbf-f033-c92b4ea2bac2"
      },
      "source": [
        "# Variables auxiliares\n",
        "tamano_entrada = 24\n",
        "h0_size = 5\n",
        "h1_size = 3\n",
        "\n",
        "# CREACIÓN DE LAS VARIABLES\n",
        "W0 = tf.Variable(np.random.randn(5, 24), dtype=tf.float64, name='W0')\n",
        "b0 = tf.Variable(np.random.randn(1,5),dtype=tf.float64, name='b0')\n",
        "W1 = tf.Variable(np.random.randn(3, 5), dtype=tf.float64, name='W1')\n",
        "b1 = tf.Variable(np.random.randn(1,3),dtype=tf.float64, name='b1')\n",
        "W2 = tf.Variable(np.random.randn(1,3), dtype=tf.float64, name='W2')\n",
        "b2 = tf.Variable(np.random.randn(1,1),dtype=tf.float64, name='b2')\n",
        "\n",
        "# Guardamos todas las variables en una lista para posteriormente acceder a ellas fácilmente\n",
        "VARIABLES = [W0, b0, W1, b1, W2, b2]\n",
        "\n",
        "\n",
        "# CREACIÓN DEL GRAFO DE OPERACIONES\n",
        "@tf.function\n",
        "def capa_sigmoide(x, W, b):\n",
        "    temp = tf.matmul(x,tf.transpose(W)) + b\n",
        "    return 1 / (1+tf.math.exp(-temp))\n",
        "\n",
        "@tf.function\n",
        "def predice(x):\n",
        "    h0 = capa_sigmoide(x, VARIABLES[0], VARIABLES[1])\n",
        "    h1 = capa_sigmoide(h0, VARIABLES[2], VARIABLES[3])\n",
        "    y = capa_sigmoide(h1, VARIABLES[4], VARIABLES[5])\n",
        "    return y\n",
        "\n",
        "# Verificación\n",
        "x_test = np.random.randn(1,tamano_entrada)\n",
        "y_pred = predice(x_test)\n",
        "print(y_pred)\n",
        "np.testing.assert_almost_equal(0.48001507, y_pred.numpy(), err_msg='Revisa tu implementación')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.48001507]], shape=(1, 1), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F-gsCDX2qej"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "El modelo declarado ya se puede utilizar para hacer predicciones pasándole a la función `predice` un tensor con datos (tal como se ha hecho en el apartado de verificación de la celda anterior). Sin embargo, como vimos en la parte 1, este modelo no está ajustado a los datos de entrada, por lo que producirá malas predicciones.\n",
        "\n",
        "Debemos encontrar un conjunto de valores para los parámetros ($\\mathbf{W}_2$, $b_2$, $\\mathbf{W}_1$, $\\mathbf{b}_1$, $\\mathbf{W}_0$ y $\\mathbf{b}_0$) que minimicen la función de coste. TensorFlow nos ayuda a optimizar este proceso.\n",
        "\n",
        "TensorFlow permite configurar el proceso de optimización, por lo que deberemos indicarle:\n",
        " 1. Qué función de pérdida queremos. En nuestro caso habíamos elegido la entropía cruzada binaria.\n",
        " 1. Qué método de optimización utilizar. Como en la parte 1, utilizaremos descenso de gradiente.\n",
        "\n",
        "Por el momento crearemos sendas variables para almacenar ambas configuraciones. Al estar organizado de esta manera, utilizar una función de pérdida distinta o un algoritmo de optimización diferente será tan sencillo como cambiar estas variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsgxywK72qen"
      },
      "source": [
        "fn_perdida = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "optimizador = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "#optimizador = tf.keras.optimizers.Adam(0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyfSrVPl2qep"
      },
      "source": [
        "### El bucle de entrenamiento\n",
        "\n",
        "El bucle de entrenamiento será análogo al utilizado en la parte 1. Consistirá en ejecutar un número preestablecido (`NUM_EPOCHS`) de pasos de entrenamiento. En cada paso haremos lo siguiente:\n",
        " 1. Tomar los datos de entrada y calcular las predicciones que hace el modelo en su estado actual\n",
        " 1. Calcular el coste (la media de las pérdidas de cada predicción)\n",
        " 1. Utilizar el valor de coste para actualizar cada variable en dirección de su gradiente\n",
        "\n",
        "Crearemos una función `paso_entrenamiento` que realice este trabajo. TensorFlow se ocupará de calcular los gradientes y realizar las actualizaciones de las variables. Para calcular los gradientes, TensorFlow utiliza un `GradientTape`. Todas las operaciones con tensores que se realicen dentro del entorno en que está declarado este `GradientTape` quedarán registradas y eso nos permitirá obtener los gradientes directamente del `GradientTape` con una simple llamada. Puedes comprobar su funcionamiento en el ejemplo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts-BnI5Y2qer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f905c5-90b7-4cd1-932e-1cd91163abb2"
      },
      "source": [
        "@tf.function\n",
        "def paso_entrenamiento(x, y):\n",
        "    # Declaración del GradientTape que registrará las operaciones\n",
        "    with tf.GradientTape() as tape:\n",
        "        # TODO - Completa la siguiente línea para que calcule las predicciones\n",
        "        y_pred = predice(x)\n",
        "\n",
        "        # Cálculo de la pérdida utilizando la función que hemos escogido anteriormente\n",
        "        perdida = fn_perdida(y, y_pred)\n",
        "\n",
        "        # Consultar los gradientes es tan sencillo como indicarle dos cosas:\n",
        "        #    1. la función cuyo gradiente queremos obtener\n",
        "        #    2. la lista de variables respecto a las cuales queremos calcular el gradiente\n",
        "        # La función nos devolverá una lista con el gradiente correspondiente a cada variable de la lista\n",
        "        gradientes = tape.gradient(perdida, VARIABLES)\n",
        "\n",
        "        # Realizar la actualización de las variables solo requiere esta llamada. Se le pasa una lista de tuplas (gradiente, variable)\n",
        "        optimizador.apply_gradients(zip(gradientes, VARIABLES))\n",
        "\n",
        "        # Para poder mostrar la tasa de acierto, la calculamos a cada paso\n",
        "        fallos = tf.abs(tf.reshape(y,(tamano_lote, 1)) - y_pred)\n",
        "        tasa_acierto = tf.reduce_sum(1 - fallos)\n",
        "\n",
        "        # Devolvemos estos dos valores para poder mostrarlos por pantalla cuando estimemos conveniente\n",
        "        return (perdida, tasa_acierto)\n",
        "\n",
        "# PROCESO DE ENTRENAMIENTO\n",
        "num_epochs = 10000\n",
        "for epoch in range(num_epochs):\n",
        "    perdida, tasa_error = paso_entrenamiento(vectores_x, etiquetas)\n",
        "\n",
        "    if epoch % 100 == 99:\n",
        "        print(\"Epoch:\", epoch, 'Pérdida:', perdida.numpy(), 'Tasa de acierto:', tasa_error.numpy()/tamano_lote)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 99 Pérdida: 0.6505565047264099 Tasa de acierto: 0.5237970073254088\n",
            "Epoch: 199 Pérdida: 0.6124920845031738 Tasa de acierto: 0.5523350376012434\n",
            "Epoch: 299 Pérdida: 0.5955331325531006 Tasa de acierto: 0.5707902847421934\n",
            "Epoch: 399 Pérdida: 0.5878661274909973 Tasa de acierto: 0.58277753544259\n",
            "Epoch: 499 Pérdida: 0.5843305587768555 Tasa de acierto: 0.590653744128563\n",
            "Epoch: 599 Pérdida: 0.5826559662818909 Tasa de acierto: 0.5958912402337152\n",
            "Epoch: 699 Pérdida: 0.5818294882774353 Tasa de acierto: 0.5994109489806961\n",
            "Epoch: 799 Pérdida: 0.581393301486969 Tasa de acierto: 0.6017976192460194\n",
            "Epoch: 899 Pérdida: 0.5811381340026855 Tasa de acierto: 0.6034290120405256\n",
            "Epoch: 999 Pérdida: 0.5809676051139832 Tasa de acierto: 0.6045529276249524\n",
            "Epoch: 1099 Pérdida: 0.5808371305465698 Tasa de acierto: 0.6053338993207833\n",
            "Epoch: 1199 Pérdida: 0.5807257294654846 Tasa de acierto: 0.6058821692911639\n",
            "Epoch: 1299 Pérdida: 0.5806236863136292 Tasa de acierto: 0.6062720881265333\n",
            "Epoch: 1399 Pérdida: 0.5805264711380005 Tasa de acierto: 0.6065540313147443\n",
            "Epoch: 1499 Pérdida: 0.5804318785667419 Tasa de acierto: 0.6067622386763698\n",
            "Epoch: 1599 Pérdida: 0.5803388357162476 Tasa de acierto: 0.6069200329338107\n",
            "Epoch: 1699 Pérdida: 0.5802468657493591 Tasa de acierto: 0.6070433236185723\n",
            "Epoch: 1799 Pérdida: 0.5801557302474976 Tasa de acierto: 0.6071429740744724\n",
            "Epoch: 1899 Pérdida: 0.5800652503967285 Tasa de acierto: 0.6072264071989948\n",
            "Epoch: 1999 Pérdida: 0.5799753665924072 Tasa de acierto: 0.6072986978677352\n",
            "Epoch: 2099 Pérdida: 0.5798860788345337 Tasa de acierto: 0.6073633175389884\n",
            "Epoch: 2199 Pérdida: 0.5797972679138184 Tasa de acierto: 0.607422642394264\n",
            "Epoch: 2299 Pérdida: 0.579708993434906 Tasa de acierto: 0.6074783003633817\n",
            "Epoch: 2399 Pérdida: 0.5796211361885071 Tasa de acierto: 0.6075314082141245\n",
            "Epoch: 2499 Pérdida: 0.5795337557792664 Tasa de acierto: 0.6075827335576343\n",
            "Epoch: 2599 Pérdida: 0.5794467926025391 Tasa de acierto: 0.6076328055386641\n",
            "Epoch: 2699 Pérdida: 0.5793601870536804 Tasa de acierto: 0.6076819904357632\n",
            "Epoch: 2799 Pérdida: 0.5792739987373352 Tasa de acierto: 0.607730543251285\n",
            "Epoch: 2899 Pérdida: 0.5791881680488586 Tasa de acierto: 0.6077786428591041\n",
            "Epoch: 2999 Pérdida: 0.5791026949882507 Tasa de acierto: 0.6078264158815169\n",
            "Epoch: 3099 Pérdida: 0.5790175795555115 Tasa de acierto: 0.6078739528345741\n",
            "Epoch: 3199 Pérdida: 0.5789327621459961 Tasa de acierto: 0.6079213189732616\n",
            "Epoch: 3299 Pérdida: 0.5788483023643494 Tasa de acierto: 0.6079685615194802\n",
            "Epoch: 3399 Pérdida: 0.5787641406059265 Tasa de acierto: 0.6080157144515205\n",
            "Epoch: 3499 Pérdida: 0.5786802768707275 Tasa de acierto: 0.6080628016918672\n",
            "Epoch: 3599 Pérdida: 0.5785967707633972 Tasa de acierto: 0.6081098392920439\n",
            "Epoch: 3699 Pérdida: 0.5785135626792908 Tasa de acierto: 0.6081568370376917\n",
            "Epoch: 3799 Pérdida: 0.578430712223053 Tasa de acierto: 0.6082037997580154\n",
            "Epoch: 3899 Pérdida: 0.5783481597900391 Tasa de acierto: 0.6082507285080845\n",
            "Epoch: 3999 Pérdida: 0.5782659649848938 Tasa de acierto: 0.6082976216977205\n",
            "Epoch: 4099 Pérdida: 0.5781840085983276 Tasa de acierto: 0.6083444761702088\n",
            "Epoch: 4199 Pérdida: 0.5781024098396301 Tasa de acierto: 0.6083912881917493\n",
            "Epoch: 4299 Pérdida: 0.5780210494995117 Tasa de acierto: 0.6084380542984569\n",
            "Epoch: 4399 Pérdida: 0.5779399871826172 Tasa de acierto: 0.6084847719564561\n",
            "Epoch: 4499 Pérdida: 0.5778591632843018 Tasa de acierto: 0.6085314400129572\n",
            "Epoch: 4599 Pérdida: 0.5777785778045654 Tasa de acierto: 0.6085780589421304\n",
            "Epoch: 4699 Pérdida: 0.5776981711387634 Tasa de acierto: 0.6086246309113508\n",
            "Epoch: 4799 Pérdida: 0.5776180028915405 Tasa de acierto: 0.6086711597068856\n",
            "Epoch: 4899 Pérdida: 0.5775379538536072 Tasa de acierto: 0.6087176505626732\n",
            "Epoch: 4999 Pérdida: 0.5774580836296082 Tasa de acierto: 0.6087641099333218\n",
            "Epoch: 5099 Pérdida: 0.5773783326148987 Tasa de acierto: 0.6088105452454372\n",
            "Epoch: 5199 Pérdida: 0.5772987008094788 Tasa de acierto: 0.6088569646525349\n",
            "Epoch: 5299 Pérdida: 0.5772191286087036 Tasa de acierto: 0.6089033768099928\n",
            "Epoch: 5399 Pérdida: 0.5771396160125732 Tasa de acierto: 0.6089497906789602\n",
            "Epoch: 5499 Pérdida: 0.5770601630210876 Tasa de acierto: 0.6089962153622929\n",
            "Epoch: 5599 Pérdida: 0.576980710029602 Tasa de acierto: 0.609042659971474\n",
            "Epoch: 5699 Pérdida: 0.5769012570381165 Tasa de acierto: 0.6090891335208469\n",
            "Epoch: 5799 Pérdida: 0.5768217444419861 Tasa de acierto: 0.6091356448439647\n",
            "Epoch: 5899 Pérdida: 0.5767422318458557 Tasa de acierto: 0.6091822025261099\n",
            "Epoch: 5999 Pérdida: 0.5766626000404358 Tasa de acierto: 0.6092288148466911\n",
            "Epoch: 6099 Pérdida: 0.5765829086303711 Tasa de acierto: 0.6092754897250462\n",
            "Epoch: 6199 Pérdida: 0.5765030384063721 Tasa de acierto: 0.6093222346629266\n",
            "Epoch: 6299 Pérdida: 0.5764230489730835 Tasa de acierto: 0.6093690566764114\n",
            "Epoch: 6399 Pérdida: 0.5763428211212158 Tasa de acierto: 0.6094159622090127\n",
            "Epoch: 6499 Pérdida: 0.5762624144554138 Tasa de acierto: 0.6094629570160718\n",
            "Epoch: 6599 Pérdida: 0.5761817097663879 Tasa de acierto: 0.6095100460079133\n",
            "Epoch: 6699 Pérdida: 0.576100766658783 Tasa de acierto: 0.6095572330352133\n",
            "Epoch: 6799 Pérdida: 0.5760194063186646 Tasa de acierto: 0.6096045205940478\n",
            "Epoch: 6899 Pérdida: 0.5759376287460327 Tasa de acierto: 0.6096519094191764\n",
            "Epoch: 6999 Pérdida: 0.5758553743362427 Tasa de acierto: 0.6096993979208396\n",
            "Epoch: 7099 Pérdida: 0.5757726430892944 Tasa de acierto: 0.6097469814004277\n",
            "Epoch: 7199 Pérdida: 0.5756892561912537 Tasa de acierto: 0.6097946509501783\n",
            "Epoch: 7299 Pérdida: 0.5756052136421204 Tasa de acierto: 0.6098423918957241\n",
            "Epoch: 7399 Pérdida: 0.5755202770233154 Tasa de acierto: 0.609890181568305\n",
            "Epoch: 7499 Pérdida: 0.5754344463348389 Tasa de acierto: 0.6099379860800457\n",
            "Epoch: 7599 Pérdida: 0.5753475427627563 Tasa de acierto: 0.6099857555950213\n",
            "Epoch: 7699 Pérdida: 0.5752593278884888 Tasa de acierto: 0.6100334172982826\n",
            "Epoch: 7799 Pérdida: 0.575169563293457 Tasa de acierto: 0.6100808647966944\n",
            "Epoch: 7899 Pérdida: 0.5750778913497925 Tasa de acierto: 0.6101279419388745\n",
            "Epoch: 7999 Pérdida: 0.5749840140342712 Tasa de acierto: 0.6101744179012437\n",
            "Epoch: 8099 Pérdida: 0.5748871564865112 Tasa de acierto: 0.6102199488593749\n",
            "Epoch: 8199 Pérdida: 0.5747866630554199 Tasa de acierto: 0.6102640203970562\n",
            "Epoch: 8299 Pérdida: 0.5746811032295227 Tasa de acierto: 0.6103058679212229\n",
            "Epoch: 8399 Pérdida: 0.574568510055542 Tasa de acierto: 0.6103443955110531\n",
            "Epoch: 8499 Pérdida: 0.5744457244873047 Tasa de acierto: 0.6103782106534763\n",
            "Epoch: 8599 Pérdida: 0.5743077993392944 Tasa de acierto: 0.610406197818442\n",
            "Epoch: 8699 Pérdida: 0.5741488337516785 Tasa de acierto: 0.6104295932088528\n",
            "Epoch: 8799 Pérdida: 0.5739675760269165 Tasa de acierto: 0.6104556794602686\n",
            "Epoch: 8899 Pérdida: 0.5737769603729248 Tasa de acierto: 0.6104974157547105\n",
            "Epoch: 8999 Pérdida: 0.5735969543457031 Tasa de acierto: 0.6105630659915366\n",
            "Epoch: 9099 Pérdida: 0.5734365582466125 Tasa de acierto: 0.61064872880524\n",
            "Epoch: 9199 Pérdida: 0.5732932686805725 Tasa de acierto: 0.6107439855673994\n",
            "Epoch: 9299 Pérdida: 0.5731620192527771 Tasa de acierto: 0.6108394422844216\n",
            "Epoch: 9399 Pérdida: 0.5730383992195129 Tasa de acierto: 0.6109289332326785\n",
            "Epoch: 9499 Pérdida: 0.5729187726974487 Tasa de acierto: 0.6110085189216721\n",
            "Epoch: 9599 Pérdida: 0.572798490524292 Tasa de acierto: 0.6110746056526807\n",
            "Epoch: 9699 Pérdida: 0.5726689100265503 Tasa de acierto: 0.61112117636757\n",
            "Epoch: 9799 Pérdida: 0.5725035667419434 Tasa de acierto: 0.6111327460511654\n",
            "Epoch: 9899 Pérdida: 0.5722010731697083 Tasa de acierto: 0.6110655282137466\n",
            "Epoch: 9999 Pérdida: 0.5717836022377014 Tasa de acierto: 0.6109910902236152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0gCVOMN2qet"
      },
      "source": [
        "El uso de TensorFlow nos ha permitido abstraernos de los detalles de implementación y del cálculo de derivadas para centrarnos en la arquitectura de nuestro modelo."
      ]
    }
  ]
}